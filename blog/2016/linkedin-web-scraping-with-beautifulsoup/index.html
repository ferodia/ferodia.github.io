
<h3 id="linkedin-data-scraping-with-beautifulsoup">Linkedin Data scraping with BeautifulSoup</h3>
<p>Today I would like to do some web scraping of Linkedin job postings, I have two ways to go:
 - Source code extraction
 - Using the Linkedin API</p>

<p>I chose the first option, mainly because the API is poorly documented and I wanted to experiment with BeautifulSoup.<br />
BeautifulSoup in few words is a library that parses HTML pages and makes it easy to extract the data.</p>

<p>Official page: https://www.crummy.com/software/BeautifulSoup/</p>

<p>```python
## Main packages needed are ulrlib2 to make url queries and beautifulSoup to structure the results
## the imports needed for this experiment
from bs4 import BeautifulSoup
import urllib2
import re
import pandas as pd
import numpy as np
# get source code of the page
def get_url(url):
    return urllib2.urlopen(url).read()</p>

<h1 id="makes-the-source--tree-format-like">makes the source  tree format like</h1>
<p>def beautify(url):
    source = get_url(url)
    return BeautifulSoup(source,”html.parser”)</p>

<p>```</p>

<p>Now that the functions are defined and libraries are imported, I’ll get job postings of linkedin<br />
The inspection of the source code of the page shows indications where to access elements we are interested in
I basically achieved that by ‘inspecting elements’ using the browser.<br />
I will look for “Data scientist” postings. Note that I’ll keep the quotes in my search because otherwise I’ll get unrelevant postings containing the words “Data” and “Scientist”.<br />
Below we are only interested to find div element with class ‘results-context’, which contains summary of the search, especially the number of items found.</p>

<p>```python
jobs = beautify(‘https://www.linkedin.com/jobs/search?keywords=%22Data+Scientist%22&amp;’
                ‘location=France&amp;trk=jobs_jserp_search_button_execute&amp;orig=JSERP&amp;locationId=fr%3A0’)</p>

<p>results_context = jobs.find(‘div’, {‘class’ : ‘results-context’}).find(‘strong’)
n_jobs = int(results_context.text.replace(‘,’,’’)) 
print “###### Number of job postings #######”
print n_jobs
print “#####################################”
```</p>

<pre><code>###### Number of job postings #######
93
#####################################
</code></pre>

<p>Now let’s check the number of postings we got on one page</p>

<p><code>python
results = jobs.find_all('li', {'class': 'job-listing'})
n_postings = len(results)
print "#### Number of job postings per page ####"
print n_postings
print "#########################################"
</code></p>

<pre><code>#### Number of job postings per page ####
25
#########################################
</code></pre>

<p><code>python
print "#### Number of pages ####"
n_pages = int(round(n_jobs/float(n_postings)))
print n_pages
print "#########################"
</code></p>

<pre><code>#### Number of pages ####
4
#########################
</code></pre>

<p>To be able to extract all postings, I need to iterate over the pages, therefore I will proceed with examining the urls of the different pages to work out the logic.</p>

<p>url of the first page
- https://www.linkedin.com/jobs/search?keywords=Data+Scientist&amp;locationId=fr:0&amp;start=0&amp;count=25&amp;trk=jobs_jserp_pagination_1</p>

<p>second page
- https://www.linkedin.com/jobs/search?keywords=Data+Scientist&amp;locationId=fr:0&amp;start=25&amp;count=25&amp;trk=jobs_jserp_pagination_2</p>

<p>third page
- https://www.linkedin.com/jobs/search?keywords=Data+Scientist&amp;locationId=fr:0&amp;start=50&amp;count=25&amp;trk=jobs_jserp_pagination_3</p>

<p>there are two elements changing :
- start=25 which is a product of page number and 25
- trk=jobs_jserp_pagination_3</p>

<p>I also noticed that the pagination number doesn’t have to be changed to go to next page, which means I can change only start value to get the next postings (may be Linkedin developers should do something about it …)</p>

<p><code>python
titles = []
companies = []
locations = []
links = []
#loop over all pages to get the posting details
for i in range(n_pages):    
    # define the base url for generic searching 
    url = ("http://www.linkedin.com/jobs/search?keywords=%22Data+Scientist%22&amp;locationId="
    "fr:0&amp;start=nPostings&amp;count=25&amp;trk=jobs_jserp_pagination_1")
    url = url.replace('nPostings',str(25*i))
    soup = beautify(url)
    # Build lists for each type of information
    results = soup.find_all('li', {'class': 'job-listing'})
    results.sort()
    # print "there are ", len(results) , " results"
    for res in results:
        # set only the value if get_text() 
        titles.append(res.h2.a.span.get_text() if res.h2.a.span else 'None')
        companies.append( res.find('span',{'class' : 'company-name-text'}).get_text() if
                         res.find('span',{'class' : 'company-name-text'}) else 'None')
        locations.append( res.find('span', {'class' : 'job-location'}).get_text() if
                        res.find('span', {'class' : 'job-location'}) else 'None' )
        links.append(res.find('a',{'class' : 'job-title-link'}).get('href') )
</code></p>

<p>As I mentioned above, all the information about where to find the job details are made easy thanks to source code viewing via any browser</p>

<p>Next, it’s time to create the data frame</p>

<p><code>python
jobs_linkedin = pd.DataFrame({'title' : titles, 'company': companies, 'location': locations, 'link' : links})
</code></p>

<p>Now the table is filled with the above columns</p>

<p><code>python
jobs_linkedin.head()
</code></p>

<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>company</th>
      <th>link</th>
      <th>location</th>
      <th>title</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Think Big, a Teradata Company</td>
      <td>https://www.linkedin.com/jobs/view/108056467?t...</td>
      <td>Antony, Île-de-France, France</td>
      <td>Data Scientist - Antony France</td>
    </tr>
    <tr>
      <th>1</th>
      <td>Disneyland Paris</td>
      <td>https://www.linkedin.com/jobs/view/136713421?t...</td>
      <td>Paris Area, France</td>
      <td>Data Scientist (Consultant Décisionnel) F/H</td>
    </tr>
    <tr>
      <th>2</th>
      <td>Allianz France</td>
      <td>https://www.linkedin.com/jobs/view/138523107?t...</td>
      <td>Région de Paris , France</td>
      <td>DATA SCIENTIST CONFIRME (E)</td>
    </tr>
    <tr>
      <th>3</th>
      <td>EY</td>
      <td>https://www.linkedin.com/jobs/view/136739066?t...</td>
      <td>Paris La Défense Cedex(FR065)</td>
      <td>Consultant Expérimenté Analytics / Data Scient...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>AXA en France</td>
      <td>https://www.linkedin.com/jobs/view/103223747?t...</td>
      <td>Nanterre, Île-de-France, France</td>
      <td>Data Scientist sénior F/H</td>
    </tr>
  </tbody>
</table>
</div>

<p>Just to verify, I can check the size of the table to make sure I got all the postings</p>

<p><code>python
jobs_linkedin.count()
</code></p>

<pre><code>company     93
link        93
location    93
title       93
dtype: int64
</code></pre>

<p>Finally, I got an actual dataset just by scraping web pages. Gathering data never have been as easy.
I can even go further by parsing the description of each posting page and extract information like
- Level
- Description
- Technologies
…</p>

<p>There are no limits to which extent we can exploit the information in HTML pages thanks to BeautifulSoup, you just have to read the documentation which is very good by the way, and get to practice on real pages.</p>

<p>Ciao!</p>
